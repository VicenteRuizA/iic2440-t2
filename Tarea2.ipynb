{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5AQEwBTxaM8",
        "outputId": "7f7f3ab6-5cc8-4d2a-eb43-e181c19d0b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.21.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark neo4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LyJfs05SBxy5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 1: Conexión a BBDD"
      ],
      "metadata": {
        "id": "GXMlPxIzMYrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import neo4j\n",
        "from neo4j import GraphDatabase\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vDhCEzZYMSTi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Conexión a la BD neo4j aura\n",
        "URI = \"neo4j+s://e52029e3.databases.neo4j.io\"\n",
        "AUTH = (\"neo4j\",\"ZckCRWcvmVzIiHrHDRd0e5ebQEJJL7iwAzjsn5uC4O0\")\n",
        "\n",
        "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
        "with driver.session() as session:\n",
        "    try:\n",
        "        session.run(\"RETURN 1\")\n",
        "        print(\"Connection to Neo4j established successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to connect to Neo4j: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QWDnjSYMXsu",
        "outputId": "e4228243-9266-4135-a670-a6ad59bf84e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection to Neo4j established successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# queries que utilizamos para probar neo4j\n",
        "\n",
        "\"\"\"\n",
        "limpiar_bd\n",
        "————————————————————\n",
        "session: sesión de conexión a bbdd\n",
        "————————————————————\n",
        "Funcion que limpia la base de datos cloud de neo4j. (actualmente tiene los mismos datos de prueba del github de la Tarea)\n",
        "return vacio.\n",
        "\"\"\"\n",
        "def limpiar_bd(session):\n",
        "    query = \"\"\"\n",
        "    MATCH (n) DETACH DELETE n\n",
        "    \"\"\"\n",
        "    session.run(query)\n",
        "\n",
        "\"\"\"\n",
        "cargar_vertices\n",
        "————————————————————\n",
        "session: sesión de conexión a bbdd\n",
        "vertices:  pandas dataframe con los vertices del grafo\n",
        "————————————————————\n",
        "Funcion que carga nodos bajo cierto formato a la base de datos cloud de neo4j.\n",
        "return vacio.\n",
        "\"\"\"\n",
        "def cargar_vertices(session, vertices):\n",
        "    query = \"\"\"\n",
        "    UNWIND $nodes AS row\n",
        "    MERGE (p:Variable {id: row})\n",
        "    \"\"\"\n",
        "    session.run(query, nodes=vertices)\n",
        "\n",
        "\"\"\"\n",
        "cargar_aristas\n",
        "————————————————————\n",
        "session: sesión de conexión a bbdd\n",
        "aristas:  pandas dataframe con las aristas del grafo\n",
        "————————————————————\n",
        "Funcion que las carga aristas bajo cierto formato a la base de datos cloud de neo4j.\n",
        "return vacio.\n",
        "\"\"\"\n",
        "def cargar_aristas(session, aristas):\n",
        "    query = \"\"\"\n",
        "    UNWIND $edges AS row\n",
        "    MATCH (p1:Variable {id: row.tail})\n",
        "    MATCH (p2:Variable {id: row.head})\n",
        "    MERGE (p1)-[:apunta {relationship_type: row.label}]->(p2);\n",
        "    \"\"\"\n",
        "\n",
        "    # se ajusta el df aristas para que se pueda procesar por neo4j\n",
        "    session.run(query, edges=aristas.to_dict(orient='records')\n",
        ")"
      ],
      "metadata": {
        "id": "kL2VX1GNG0PM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Considerando una flecha, la dirección es desde tail hasta head.\n",
        "\n",
        "\"\"\"\n",
        "get_all_relationships_bd\n",
        "————————————————————\n",
        "session: sesión de conexión a bbdd\n",
        "sc: contexto de spark\n",
        "————————————————————\n",
        "Funcion que busca aristas de neo4j cloud. Notar que los datos cargados se\n",
        "procesan considerando el formato de las functiones de carga definidas previamente\n",
        "return todas las aristas del grafo como un rdd.\n",
        "\"\"\"\n",
        "def csv_grafo(session, csv):\n",
        "    aristas = pd.read_csv(csv, header=None,names=['tail','label','head'])\n",
        "    nodos = pd.concat([aristas['tail'],aristas['head']]).unique()\n",
        "    cargar_vertices(session,nodos)\n",
        "    cargar_aristas(session,aristas)\n",
        "    query = \"MATCH (n) RETURN n LIMIT 5\"\n",
        "    grafo = session.run(query)\n",
        "    return grafo\n",
        "\n",
        "# with driver.session() as session:\n",
        "#     grafo = csv_grafo(session,'data.csv')"
      ],
      "metadata": {
        "id": "IGy32wnDTEhp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "get_all_relationships_bd\n",
        "————————————————————\n",
        "session: sesión de conexión a bbdd\n",
        "sc: contexto de spark\n",
        "————————————————————\n",
        "Funcion que busca aristas de neo4j cloud. Notar que los datos cargados se\n",
        "procesan considerando el formato de las functiones de carga definidas previamente\n",
        "return todas las aristas del grafo como un rdd.\n",
        "\"\"\"\n",
        "def get_all_relationships_bd(session, sc):\n",
        "  query = \"\"\"\n",
        "  MATCH (p1:Variable)-[r]->(p2:Variable)\n",
        "  RETURN p1.id AS tail, p2.id AS head, r.relationship_type AS label\n",
        "  \"\"\"\n",
        "  # notar que result tiene un grafo de neo4j\n",
        "  result = session.run(query)\n",
        "  records = [(record[\"tail\"], record[\"label\"], record[\"head\"]) for record in result]\n",
        "  return sc.parallelize(records)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "get_all_relationships_graph\n",
        "————————————————————\n",
        "graph: grafo de neo4j\n",
        "sc: contexto de spark\n",
        "————————————————————\n",
        "Funcion para transformar directamente un grafo.\n",
        "return grafo del formato esperado como un rdd de pyspark.\n",
        "\"\"\"\n",
        "def get_all_relationships_graph(graph, sc):\n",
        "    records = [(record[\"tail\"], record[\"label\"], record[\"head\"]) for record in graph]\n",
        "    return sc.parallelize(records)"
      ],
      "metadata": {
        "id": "euwUS4pqibB-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 2: Búsqueda de triángulos dirigidos con una etiqueta fija"
      ],
      "metadata": {
        "id": "CIu7lZhSouOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "hashFunction\n",
        "————————————————————\n",
        "x: datos a hashear\n",
        "b: parametro b\n",
        "————————————————————\n",
        "Funcion de hash con x modulo b\n",
        "return numero hasheado\n",
        "\"\"\"\n",
        "def hashFunction(x, b):\n",
        "  return x % b\n",
        "\n",
        "\"\"\"\n",
        "keyGenerator\n",
        "————————————————————\n",
        "x: datos a generar las llaves\n",
        "b: parametro b\n",
        "————————————————————\n",
        "Generacion de cada una de las combinaciones posibles de\n",
        "(b1,b2,0) ... (b1,b2,b-1), (0,b1,b2) ... (b-1,b1,b2), (b1,0,b2) ... (b1,b-1,b2)\n",
        "return todas las llaves generadas para x\n",
        "\"\"\"\n",
        "def keyGenerator(x, b):\n",
        "  keys = []\n",
        "  keys += [((x[0][0], x[0][1], i), x[1]) for i in range(b)]\n",
        "  keys += [((i, x[0][0], x[0][1]), x[1]) for i in range(b)]\n",
        "  keys += [((x[0][1], i, x[0][0]), x[1]) for i in range(b)]\n",
        "  return keys"
      ],
      "metadata": {
        "id": "7L4TgsbPolxt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "triangleFinder\n",
        "————————————————————\n",
        "data: rdd ya paralelizada con todas las aristas\n",
        "b: parametro b\n",
        "label: etiqueta fija (ejemplo es 11)\n",
        "————————————————————\n",
        "Funcion que implementa el algoritmo de la pregunta 2\n",
        "return todas las combinaciones de triangulos dirigidos dentro del grafo\n",
        "\"\"\"\n",
        "def triangleFinder(data, b, label):\n",
        "  dataFiltered = data.filter(lambda x: x[1]  == label)\n",
        "  hashedData = dataFiltered.map(lambda x: ((hashFunction(x[0], b), hashFunction(x[2], b)), x))\n",
        "  expandedData = hashedData.flatMap(lambda x: keyGenerator(x, b))\n",
        "  groupedData = expandedData.groupByKey().mapValues(list).collect()\n",
        "\n",
        "  output_set = set()\n",
        "\n",
        "  for _, i in groupedData:\n",
        "      for x_u, _,y_u in i:\n",
        "          for x_v,_, y_v in i:\n",
        "              for x_w,_, y_w in i:\n",
        "                  if (y_u == x_v) and (y_v == x_w) and (y_w == x_u):\n",
        "                      triplet = (x_u, x_v, x_w)\n",
        "                      output_set.add(triplet)\n",
        "\n",
        "  return list(output_set)"
      ],
      "metadata": {
        "id": "h2Gz2VMxFWFr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 3: Búsqueda de cuadrados dirigidos con etiqueta variable"
      ],
      "metadata": {
        "id": "Agk0vjDx7tyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "inputArista\n",
        "————————————————————\n",
        "A: arreglo de nodos unicos\n",
        "L: arraglo de etiquetas unicas\n",
        "M: matriz de adyacencia de la aristas\n",
        "————————————————————\n",
        "Convierte la matriz de adyacencia en una lista de aristas\n",
        "return lista de aristas\n",
        "\"\"\"\n",
        "def inputArista(A, L, M):\n",
        "    return [(A[i], L[j], A[k]) for i in range(len(A)) for j in range(len(L)) for k in range(len(A)) if M[i][j][k] == 1]\n",
        "\n",
        "\"\"\"\n",
        "keyGenerator\n",
        "————————————————————\n",
        "x: datos a generar las llaves\n",
        "b: parametro b\n",
        "consulta: lista de aristas\n",
        "————————————————————\n",
        "Generacion de cada una de las combinaciones posibles de (b1,b2,0,0) -> (b1,b2,b-1,b-1), etc..\n",
        "Las diferencias con keyGenerator son:\n",
        "   - la limitación de las etiquetas\n",
        "   - la cantidad de combinaciones posibles\n",
        "   - el tamaño de las llaves\n",
        "return todas las llaves generadas para x\n",
        "\"\"\"\n",
        "def keyGenerator4(x, b, consulta):\n",
        "    keys = []\n",
        "    if x[1][1] == consulta[0][1]:\n",
        "        keys += [((x[0][0], x[0][1], i, j), x[1]) for i in range(b) for j in range(b)]\n",
        "    if x[1][1] == consulta[1][1]:\n",
        "        keys += [((j, x[0][0], x[0][1], i), x[1]) for i in range(b) for j in range(b)]\n",
        "    if x[1][1] == consulta[2][1]:\n",
        "        keys += [((i, j, x[0][0], x[0][1]), x[1]) for i in range(b) for j in range(b)]\n",
        "    if x[1][1] == consulta[3][1]:\n",
        "        keys += [((x[0][1], i, j, x[0][0]), x[1]) for i in range(b) for j in range(b)]\n",
        "    return keys\n",
        "\n",
        "\"\"\"\n",
        "dataTransform\n",
        "————————————————————\n",
        "item: llave y valor\n",
        "————————————————————\n",
        "genera un diccionario para cada una de las etiquetas de una llave\n",
        "return llave y diccionario de etiquetas\n",
        "\"\"\"\n",
        "def dataTransform(item):\n",
        "    key, values = item\n",
        "    result = {}\n",
        "    for (x, label, y) in values:\n",
        "        if label not in result:\n",
        "            result[label] = []\n",
        "        result[label].append((x, y))\n",
        "    return key, list(result.items())"
      ],
      "metadata": {
        "id": "g5GLkGyw7yCy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "squareFinder\n",
        "————————————————————\n",
        "data: rdd ya paralelizada con todas las aristas\n",
        "b: parametro b\n",
        "A: arreglo de nodos unicos\n",
        "L: arraglo de etiquetas unicas\n",
        "M: matriz de adyacencia de la aristas\n",
        "————————————————————\n",
        "Funcion que implementa el algoritmo de la pregunta 3\n",
        "return todas las combinaciones de cuadrados dirigidos dentro del grafo que cumplen la consulta\n",
        "\"\"\"\n",
        "def squareFinder(data, b, A, L, M):\n",
        "  query = inputArista(A, L, M)\n",
        "  hashedData4 = data.map(lambda x: ((hashFunction(x[0], b), hashFunction(x[2], b)), x))\n",
        "  expandedData4 = hashedData4.flatMap(lambda x: keyGenerator4(x, b, query))\n",
        "  groupedData4 = expandedData4.groupByKey().mapValues(list)\n",
        "\n",
        "  transformed_rdd4 = groupedData4.map(dataTransform)\n",
        "\n",
        "  last_filter4 = transformed_rdd4.filter(lambda x: len(x[1]) == len(L)).collect()\n",
        "\n",
        "  results = []\n",
        "\n",
        "  for _, i in last_filter4:\n",
        "    edges = dict(i)\n",
        "    for x_u, y_u in edges[query[0][1]]:\n",
        "      for x_v, y_v in edges[query[1][1]]:\n",
        "        for x_w, y_w in edges[query[2][1]]:\n",
        "          for x_t, y_t in edges[query[3][1]]:\n",
        "            if (y_u == x_v) and (y_v == x_w) and (y_w == x_t) and (y_t == x_u):\n",
        "              results.append((x_u, x_v, x_w, x_t))\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "pwumtNf9Gvu-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aux_find_squares(item, query):\n",
        "    key, edges_list = item\n",
        "    edges = dict(edges_list)\n",
        "    results = []\n",
        "    for x_u, y_u in edges.get(query[0][1], []):\n",
        "        for x_v, y_v in edges.get(query[1][1], []):\n",
        "            for x_w, y_w in edges.get(query[2][1], []):\n",
        "                for x_t, y_t in edges.get(query[3][1], []):\n",
        "                    if (y_u == x_v) and (y_v == x_w) and (y_w == x_t) and (y_t == x_u):\n",
        "                        results.append((x_u, x_v, x_w, x_t))\n",
        "    return results\n",
        "\n",
        "def squareFinder3000(data, b, A, L, M):\n",
        "    query = inputArista(A, L, M)\n",
        "    broadcast_query = sc.broadcast(query)\n",
        "\n",
        "    hashedData4 = data.map(lambda x: ((hashFunction(x[0], b), hashFunction(x[2], b)), x))\n",
        "    expandedData4 = hashedData4.flatMap(lambda x: keyGenerator4(x, b, broadcast_query.value))\n",
        "\n",
        "    groupedData4 = expandedData4.groupByKey().mapValues(list)\n",
        "    transformed_rdd4 = groupedData4.map(dataTransform)\n",
        "\n",
        "    last_filter4 = transformed_rdd4.filter(lambda x: len(x[1]) == len(L))\n",
        "\n",
        "    results = last_filter4.flatMap(lambda item: aux_find_squares(item, query)).collect()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "8D_MR0H24IMm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruebas\n",
        "\n",
        "Para realizar las pruebas generamos archivos txt (para la pregunta 2 y 3) en las que se formatean los datos como se especifican en cada sub-sección."
      ],
      "metadata": {
        "id": "Wk20NevZN1M1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregunta 1\n",
        "\n",
        "Haz tu esta parte Vicho (y las funciones de la 1, no cacho nada xd)\n",
        "Idea: correr ```triangleFinder``` con los datos de Neo4J."
      ],
      "metadata": {
        "id": "rORCJBPRQEC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc\n",
        "\n",
        "with driver.session() as session:\n",
        "    data = get_all_relationships_bd(session, sc)\n",
        "\n",
        "print(triangleFinder(data, 10, 11))"
      ],
      "metadata": {
        "id": "3q8Rx1yfQFvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df094df-3360-4fdf-bbe8-361209ab8a2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 3, 4), (3, 4, 2), (2, 3, 4), (3, 4, 1), (4, 1, 3), (4, 2, 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregunta 2\n",
        "\n",
        "Los archivos contienen 3 lineas que representan cada uno de los parámetros de la función ```triangleFinder```. \\\n",
        "```data: [(x1,a1,y1),(x2,a2,y2),...,(xn-1,am-1,yn-1),(xn,am,yn)]``` \\\n",
        "```b: int``` \\\n",
        "```label: int```"
      ],
      "metadata": {
        "id": "6hqCyJPrP-U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readFile2(filename):\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"TriangleCounting\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    sc = spark.sparkContext\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = eval(lines[0].strip())\n",
        "    b = int(lines[1].strip())\n",
        "    label = int(lines[2].strip())\n",
        "\n",
        "    return sc.parallelize(data), b, label"
      ],
      "metadata": {
        "id": "TPw-IZdbP9mG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, b, label = readFile2('datos_p2.txt')\n",
        "print(triangleFinder(data, b, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhnOyiZxX15u",
        "outputId": "28230485-25ac-4e35-cfd5-bb2a67e4e501"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 3, 4), (3, 4, 2), (2, 3, 4), (3, 4, 1), (4, 1, 3), (4, 2, 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregunta 3\n",
        "\n",
        "Los archivos contienen 5 lineas que representan cada uno de los parámetros de la función ```squareFinder```. \\\n",
        "```data: [(x1,a1,y1),(x2,a2,y2),...,(xn-1,am-1,yn-1),(xn,am,yn)]``` \\\n",
        "```b: int``` \\\n",
        "```A: [a1,a2,...,ap-1,ap]``` \\\n",
        "```L: [l1,l2,...,lq-1,lq]``` \\\n",
        "```M: [[[0,0,...,1,0],...,[0,0,...,0,1]]]```\n"
      ],
      "metadata": {
        "id": "brqwr8qPP_k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readFile3(filename):\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"TriangleCounting\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    sc = spark.sparkContext\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = eval(lines[0].strip())\n",
        "    b = int(lines[1].strip())\n",
        "    A = eval(lines[2].strip())\n",
        "    L = eval(lines[3].strip())\n",
        "    M = eval(lines[4].strip())\n",
        "\n",
        "    return sc.parallelize(data), b, A, L, M"
      ],
      "metadata": {
        "id": "Sbz2X70TS0L6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time"
      ],
      "metadata": {
        "id": "ubYR8oET6M7I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time()\n",
        "data, b, A, L, M = readFile3('datos_p3.txt')\n",
        "print(\"--- %s seconds ---\" % (time() - start_time))\n",
        "print(squareFinder(data, b, A, L, M))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08h7bfpsN4E7",
        "outputId": "9bd3269c-c385-4b04-8a45-0ba73917d339"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.02127218246459961 seconds ---\n",
            "[(1, 5, 10, 17)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time()\n",
        "data, b, A, L, M = readFile3('datos_p3.txt')\n",
        "print(\"--- %s seconds ---\" % (time() - start_time))\n",
        "print(squareFinder3000(data, b, A, L, M))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9Bx8ZUa6sEl",
        "outputId": "0b6eca99-6328-4862-919a-68bec1b1b868"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.02088475227355957 seconds ---\n",
            "[(1, 5, 10, 17)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODlwHjWX7tCN",
        "outputId": "f0e11eca-0744-4451-8df7-887a4861ed04"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time()\n",
        "data, b, A, L, M = readFile3('datos_p3_2.txt')\n",
        "print(\"--- %s seconds ---\" % (time() - start_time))\n",
        "print(squareFinder(data, b, A, L, M))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJcVTP3VTqzH",
        "outputId": "b6b5ae6a-5df1-4eb5-e182-69720146d75c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.021589040756225586 seconds ---\n",
            "[(2, 3, 4, 5), (1, 3, 4, 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time()\n",
        "data, b, A, L, M = readFile3('datos_p3_2.txt')\n",
        "print(\"--- %s seconds ---\" % (time() - start_time))\n",
        "print(squareFinder3000(data, b, A, L, M))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5aMB9f26EWg",
        "outputId": "ba7de565-565d-4a80-cd0f-886598a5dc4d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.031281232833862305 seconds ---\n",
            "[(2, 3, 4, 5), (1, 3, 4, 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soztJh197U1B",
        "outputId": "0fb3977f-e82f-47e4-bfce-11c68a574d1f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    }
  ]
}